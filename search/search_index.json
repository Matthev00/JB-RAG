{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CODE RAG documentation!","text":""},{"location":"#description","title":"Description","text":"<p>Retrieval-Augmented Generation (RAG) system over a code repository for a question-answering task.</p> <p>Users can use the system for question answering over the repository: - Input: Natural language query (question). - Output: Relevant code locations (files).  </p>"},{"location":"#system","title":"System","text":"<p>Code is parsed into chunks using a custom CodeParser, and embeddings are generated with a SentenceTransformer model. The FAISS index enables fast similarity-based searches, supporting both radius-based and top-k retrieval methods. The system is designed to be modular, allowing easy customization of embedding models and repositories, while ensuring reproducibility through experiment tracking with Optuna and Weights &amp; Biases.</p>"},{"location":"#llm-summaries","title":"LLM summaries","text":"<p>To further enhance user experience, I integrated a local LLM (Large Language Model) to generate natural language summaries of the retrieved results. This feature is available in the web interface and can be optionally enabled using a checkbox.</p> <p>The specific LLM model and device configuration can be easily adjusted in the <code>src/config.py</code> file via the <code>SUMMARY_MODEL</code> and <code>DEVICE</code> variables.</p>"},{"location":"#commands","title":"Commands","text":"<p>The Makefile contains the central entry points for common tasks related to this project.</p>"},{"location":"commands/","title":"Commands","text":""},{"location":"commands/#crate-enviroment","title":"Crate Enviroment","text":"<pre><code>make create_environment\n</code></pre>"},{"location":"commands/#install-python-dependencies","title":"Install Python Dependencies","text":"<p>Option A \u2013 Basic setup (no LLM)</p> <pre><code>make requirements\n</code></pre> <p>Option B \u2013 Full setup with LLM</p> <pre><code>make requirements-llm\n</code></pre> <p>This will also download the model defined in src/config.py (e.g. tiiuae/falcon-7b-instruct). This might take some time depending on your internet speed. You only need this if you want to use full LLM features like query explanations. If you're only interested in path-based retrieval or running evaluations \u2014 you can skip this step and just run make requirements.</p>"},{"location":"commands/#delete-all-compiled-python-files","title":"Delete all compiled Python files","text":"<p>Deletes all compiled Python files (.pyc, .pyo) and pycache directories.</p> <pre><code>make clean\n</code></pre>"},{"location":"commands/#lint-using-flake8-and-black","title":"Lint using flake8 and black","text":"<p>Lints the source code using flake8 and black. Use make format to format the code.</p> <pre><code>make lint\n</code></pre>"},{"location":"commands/#format-source-code-with-black","title":"Format source code with black","text":"<p>Formats the source code using black.</p> <pre><code>make format\n</code></pre>"},{"location":"commands/#prepare-knowladge-base","title":"Prepare Knowladge Base","text":"<pre><code>make prepare_kb\n</code></pre>"},{"location":"commands/#have-fun-with-code-rag","title":"Have fun with CODE RAG","text":"<pre><code>streamlit run src/main.py --server.fileWatcherType=none\n</code></pre>"},{"location":"commands/#replicate-experiment","title":"Replicate Experiment","text":"<p>Evaluate on validation data 1. Download file from here 2. Run <code>make evaluate</code>.</p>"},{"location":"commands/#replicate-experiment_1","title":"Replicate Experiment","text":"<p>Replicates my experiments  For this you need an acount on Weights &amp; Biases</p> <pre><code>make hyperparameter_experiment\n</code></pre> <pre><code>make query_expansion_experiment\n</code></pre>"},{"location":"commands/#serve-documentation","title":"Serve documentation","text":"<p>Serves the documentation locally using mkdocs.</p> <pre><code>make docs_serve\n</code></pre>"},{"location":"commands/#build-documentation","title":"Build documentation","text":"<p>Builds the documentation using mkdocs.</p> <pre><code>make docs_build\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before starting with the project, make sure you have installed all the required dependencies. You can do this by running the following command:</p> <pre><code>make create_environment\n</code></pre> <p>Option A \u2013 Basic setup (no LLM)</p> <pre><code>make requirements\n</code></pre> <p>Option B \u2013 Full setup with LLM</p> <pre><code>make requirements-llm\n</code></pre> <p>This will also download the model defined in src/config.py (e.g. tiiuae/falcon-7b-instruct). This might take some time depending on your internet speed. You only need this if you want to use full LLM features like query explanations. If you're only interested in path-based retrieval or running evaluations \u2014 you can skip this step and just run <code>make requirements</code>.</p>"},{"location":"getting-started/#step-1-prepare-knowladge-base","title":"Step 1: Prepare Knowladge Base","text":"<pre><code>make prepare_kb\n</code></pre>"},{"location":"getting-started/#step-2-inference-with-system","title":"Step 2: Inference with System","text":"<p>run</p> <pre><code>uv run src/main.py\n</code></pre> <p>Now you can input your question, and the system will return an answer with paths to the relevant files.</p>"},{"location":"getting-started/#step-3optional-replicate-experiment","title":"Step 3:(Optional) Replicate Experiment","text":"<p>You can reproduce experiments by preparing validation dataset by downloading file from here and saving it as <code>/data/escrcpy_val.json</code>. Then you can run <code>make hyperparameter_experiment</code> and <code>make query_expansion_experiment</code> or even <code>make reranker_experiment</code>. Important - before running experiment you need to do step 1 Prepare Knowladge Base</p>"},{"location":"api/evaluation/","title":"Evaluation","text":""},{"location":"api/evaluation/#src.evaluation.dataset.RAGDataset","title":"<code>RAGDataset</code>","text":"Source code in <code>src/evaluation/dataset.py</code> <pre><code>class RAGDataset:\n    def __init__(self, data_path: Path) -&gt; None:\n        \"\"\"\n        Initializes the RAGDataset object with the path to the dataset.\"\n        \"\"\"\n        self.data_path = data_path\n        self.data = self.load_data()\n\n    def load_data(self) -&gt; list[dict]:\n        \"\"\"\n        Loads the data from the dataset.\n\n        Returns:\n            list: List of dictionaries with the data.\n        \"\"\"\n        with open(self.data_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        return data\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: Length of the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"\n        Returns the item at the specified index.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            dict: Item at the specified index.\n        \"\"\"\n        return self.data[idx][\"question\"], set(self.data[idx][\"files\"])\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.dataset.RAGDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the item at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Item at the specified index.</p> Source code in <code>src/evaluation/dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"\n    Returns the item at the specified index.\n\n    Args:\n        idx (int): Index of the item.\n\n    Returns:\n        dict: Item at the specified index.\n    \"\"\"\n    return self.data[idx][\"question\"], set(self.data[idx][\"files\"])\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.dataset.RAGDataset.__init__","title":"<code>__init__(data_path)</code>","text":"<p>Initializes the RAGDataset object with the path to the dataset.\"</p> Source code in <code>src/evaluation/dataset.py</code> <pre><code>def __init__(self, data_path: Path) -&gt; None:\n    \"\"\"\n    Initializes the RAGDataset object with the path to the dataset.\"\n    \"\"\"\n    self.data_path = data_path\n    self.data = self.load_data()\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.dataset.RAGDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Length of the dataset.</p> Source code in <code>src/evaluation/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: Length of the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.dataset.RAGDataset.load_data","title":"<code>load_data()</code>","text":"<p>Loads the data from the dataset.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[dict]</code> <p>List of dictionaries with the data.</p> Source code in <code>src/evaluation/dataset.py</code> <pre><code>def load_data(self) -&gt; list[dict]:\n    \"\"\"\n    Loads the data from the dataset.\n\n    Returns:\n        list: List of dictionaries with the data.\n    \"\"\"\n    with open(self.data_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    return data\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.evaluator.RAGEvaluator","title":"<code>RAGEvaluator</code>","text":"Source code in <code>src/evaluation/evaluator.py</code> <pre><code>class RAGEvaluator:\n    @staticmethod\n    def evaluate(retriever: FAISSRetriever, dataset: RAGDataset, **retriever_params):\n        \"\"\"\n        Evaluates the retriever on the given dataset.\n\n        Args:\n            retriever (FAISSRetriever): Retriever object.\n            dataset (RAGDataset): Dataset object.\n            retriever_params: Parameters for the retriever.\n\n        Returns:\n            dict: Evaluation results.\n        \"\"\"\n        precision_scores = []\n        recall_scores = []\n        f1_scores = []\n        mrr_scores = []\n\n        for question, expected_files in dataset:\n            results = retriever.search(question, **retriever_params)\n            top_10_results = results[:10]\n            retrieved_files = set([res[\"relative_path\"] for res in top_10_results])\n\n            # Precision@10\n            precision = (\n                len(retrieved_files &amp; expected_files) / len(retrieved_files)\n                if retrieved_files\n                else 0\n            )\n            precision_scores.append(precision)\n\n            # Recall@10\n            recall = (\n                len(retrieved_files &amp; expected_files) / len(expected_files)\n                if expected_files\n                else 0\n            )\n            recall_scores.append(recall)\n\n            # F1@10\n            f1 = (\n                (2 * precision * recall) / (precision + recall)\n                if (precision + recall) &gt; 0\n                else 0\n            )\n            f1_scores.append(f1)\n\n            # MRR (Mean Reciprocal Rank)\n            rank = next(\n                (\n                    i + 1\n                    for i, res in enumerate(top_10_results)\n                    if res[\"relative_path\"] in expected_files\n                ),\n                0,\n            )\n            mrr_scores.append(1 / rank if rank &gt; 0 else 0)\n        return {\n            \"Precision@10\": sum(precision_scores) / len(precision_scores),\n            \"Recall@10\": sum(recall_scores) / len(recall_scores),\n            \"F1@10\": sum(f1_scores) / len(f1_scores),\n            \"MRR\": sum(mrr_scores) / len(mrr_scores),\n        }\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.evaluator.RAGEvaluator.evaluate","title":"<code>evaluate(retriever, dataset, **retriever_params)</code>  <code>staticmethod</code>","text":"<p>Evaluates the retriever on the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>retriever</code> <code>FAISSRetriever</code> <p>Retriever object.</p> required <code>dataset</code> <code>RAGDataset</code> <p>Dataset object.</p> required <code>retriever_params</code> <p>Parameters for the retriever.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Evaluation results.</p> Source code in <code>src/evaluation/evaluator.py</code> <pre><code>@staticmethod\ndef evaluate(retriever: FAISSRetriever, dataset: RAGDataset, **retriever_params):\n    \"\"\"\n    Evaluates the retriever on the given dataset.\n\n    Args:\n        retriever (FAISSRetriever): Retriever object.\n        dataset (RAGDataset): Dataset object.\n        retriever_params: Parameters for the retriever.\n\n    Returns:\n        dict: Evaluation results.\n    \"\"\"\n    precision_scores = []\n    recall_scores = []\n    f1_scores = []\n    mrr_scores = []\n\n    for question, expected_files in dataset:\n        results = retriever.search(question, **retriever_params)\n        top_10_results = results[:10]\n        retrieved_files = set([res[\"relative_path\"] for res in top_10_results])\n\n        # Precision@10\n        precision = (\n            len(retrieved_files &amp; expected_files) / len(retrieved_files)\n            if retrieved_files\n            else 0\n        )\n        precision_scores.append(precision)\n\n        # Recall@10\n        recall = (\n            len(retrieved_files &amp; expected_files) / len(expected_files)\n            if expected_files\n            else 0\n        )\n        recall_scores.append(recall)\n\n        # F1@10\n        f1 = (\n            (2 * precision * recall) / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0\n        )\n        f1_scores.append(f1)\n\n        # MRR (Mean Reciprocal Rank)\n        rank = next(\n            (\n                i + 1\n                for i, res in enumerate(top_10_results)\n                if res[\"relative_path\"] in expected_files\n            ),\n            0,\n        )\n        mrr_scores.append(1 / rank if rank &gt; 0 else 0)\n    return {\n        \"Precision@10\": sum(precision_scores) / len(precision_scores),\n        \"Recall@10\": sum(recall_scores) / len(recall_scores),\n        \"F1@10\": sum(f1_scores) / len(f1_scores),\n        \"MRR\": sum(mrr_scores) / len(mrr_scores),\n    }\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.query_expansion_experiment.main","title":"<code>main()</code>","text":"<p>Script for running Query Expansion experiments.</p> Source code in <code>src/evaluation/query_expansion_experiment.py</code> <pre><code>def main():\n    \"\"\"\n    Script for running Query Expansion experiments.\n    \"\"\"\n    set_seeds(42)\n\n    dataset = RAGDataset(Path(\"data/escrcpy_val.json\"))\n\n    def evaluate_query_expansion(\n        expand_query_type: str, retriever_params: dict, trial_number\n    ):\n        \"\"\"\n        Evaluates the system with a specific Query Expansion type.\n\n        Args:\n            expand_query_type (str): Type of Query Expansion (None, candidate_terms, wordnet).\n            retriever_params (dict): Parameters for the retriever (radius or top_k).\n            trail_number (int): idx of trail\n\n        Returns:\n            dict: Evaluation results including quality metrics and latency.\n        \"\"\"\n        wandb.init(\n            project=\"JB-RAG-query-expansion\",\n            name=f\"trial-{trial_number}\",\n            reinit=True,\n        )\n        retriever = FAISSRetriever(EMBEDDING_MODEL)\n        retriever.load_index(\"escrcpy\")\n\n        start_time = time.time()\n        results = RAGEvaluator.evaluate(\n            retriever, dataset, expand_query_type=expand_query_type, **retriever_params\n        )\n        latency = time.time() - start_time\n\n        wandb.log(\n            {\n                \"expand_query_type\": expand_query_type,\n                \"latency\": latency,\n                \"query_top_k\": (\n                    retriever_params.get(\"query_top_k\")\n                    if expand_query_type == \"candidate_terms\"\n                    else None\n                ),\n                \"radius\": (retriever_params.get(\"radius\")),\n                \"Precision@10\": results[\"Precision@10\"],\n                \"Recall@10\": results[\"Recall@10\"],\n                \"F1@10\": results[\"F1@10\"],\n            }\n        )\n\n        return results\n\n    query_expansion_types = [None, \"candidate_terms\", \"wordnet\"]\n    retriever_configs = [{\"top_k\": 11, \"query_top_k\": i} for i in range(20)]\n    trial_number = 1\n    for expand_query_type in query_expansion_types:\n        for retriever_params in retriever_configs:\n            evaluate_query_expansion(expand_query_type, retriever_params, trial_number)\n            trial_number += 1\n\n    wandb.finish()\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.hyperparameters_experiment.main","title":"<code>main()</code>","text":"<p>Script for running the optimization experiment with Optuna.</p> Source code in <code>src/evaluation/hyperparameters_experiment.py</code> <pre><code>def main():\n    \"\"\"\n    Script for running the optimization experiment with Optuna.\n    \"\"\"\n    set_seeds(42)\n    wandb.init(project=\"JB-RAG-optimization\", name=\"optuna-experiment\")\n\n    dataset = RAGDataset(Path(\"data/escrcpy_val.json\"))\n\n    def objective(trial):\n        \"\"\"\n        Objective function for Optuna to optimize parameters.\n        \"\"\"\n        wandb.init(\n            project=\"JB-RAG-optimization\",\n            name=f\"trial-{trial.number}\",\n            config={\"trial_number\": trial.number},\n            reinit=True,\n        )\n\n        max_chunk_size = trial.suggest_int(\"max_chunk_size\", 10, 120)\n        use_radius = trial.suggest_categorical(\"use_radius\", [True, False])\n        if use_radius:\n            radius = trial.suggest_float(\"radius\", 0.05, 1.0)\n            top_k = None\n        else:\n            radius = None\n            top_k = trial.suggest_int(\"top_k\", 1, 50)\n\n        prepare_knowledge_base(max_chunk_size, \"data/repos/escrcpy\")\n        retriever = FAISSRetriever(EMBEDDING_MODEL)\n        retriever.build_index(\"escrcpy\")\n        retriever.load_index(\"escrcpy\")\n\n        retriever_params = {\"radius\": radius, \"top_k\": top_k}\n        results = RAGEvaluator.evaluate(retriever, dataset, **retriever_params)\n\n        wandb.log(\n            {\n                \"max_chunk_size\": max_chunk_size,\n                \"radius\": radius,\n                \"top_k\": top_k,\n                \"Precision@10\": results[\"Precision@10\"],\n                \"Recall@10\": results[\"Recall@10\"],\n                \"F1@10\": results[\"F1@10\"],\n                \"MRR\": results[\"MRR\"],\n            }\n        )\n        wandb.finish()\n\n        return results[\"Recall@10\"]\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(\"Best parameters:\", study.best_params)\n    print(\"Best Recall@10:\", study.best_value)\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.utils.set_seeds","title":"<code>set_seeds(seed)</code>","text":"<p>Set seeds for reproducibility.</p> Source code in <code>src/evaluation/utils.py</code> <pre><code>def set_seeds(seed: int) -&gt; None:\n    \"\"\"\n    Set seeds for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/preprocessing/","title":"Data Preprocessing","text":""},{"location":"api/preprocessing/#src.preprocessing.repo_loader.RepoLoader","title":"<code>RepoLoader</code>","text":"Source code in <code>src/preprocessing/repo_loader.py</code> <pre><code>class RepoLoader:\n    def __init__(self, repo_url: str):\n        \"\"\"\n        Class for downloding repo from external source.\n\n        Args:\n            repo_url (str): URL of the repository.\n        \"\"\"\n        self.repo_url = repo_url\n        self.repo_dir = Path(REPO_DIR) / Path(\n            repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n        )\n\n    def clone(self) -&gt; Repo:\n        \"\"\"\n        Clones the repository from the URL to the local directory.\n        Path is specified in config.py file as REPO_DIR.\n\n        Returns:\n            Repo: GitPython Repo object.\n        \"\"\"\n        if self.repo_dir.exists():\n            return Repo(self.repo_dir)\n        else:\n            return Repo.clone_from(self.repo_url, self.repo_dir)\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.repo_loader.RepoLoader.__init__","title":"<code>__init__(repo_url)</code>","text":"<p>Class for downloding repo from external source.</p> <p>Parameters:</p> Name Type Description Default <code>repo_url</code> <code>str</code> <p>URL of the repository.</p> required Source code in <code>src/preprocessing/repo_loader.py</code> <pre><code>def __init__(self, repo_url: str):\n    \"\"\"\n    Class for downloding repo from external source.\n\n    Args:\n        repo_url (str): URL of the repository.\n    \"\"\"\n    self.repo_url = repo_url\n    self.repo_dir = Path(REPO_DIR) / Path(\n        repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n    )\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.repo_loader.RepoLoader.clone","title":"<code>clone()</code>","text":"<p>Clones the repository from the URL to the local directory. Path is specified in config.py file as REPO_DIR.</p> <p>Returns:</p> Name Type Description <code>Repo</code> <code>Repo</code> <p>GitPython Repo object.</p> Source code in <code>src/preprocessing/repo_loader.py</code> <pre><code>def clone(self) -&gt; Repo:\n    \"\"\"\n    Clones the repository from the URL to the local directory.\n    Path is specified in config.py file as REPO_DIR.\n\n    Returns:\n        Repo: GitPython Repo object.\n    \"\"\"\n    if self.repo_dir.exists():\n        return Repo(self.repo_dir)\n    else:\n        return Repo.clone_from(self.repo_url, self.repo_dir)\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.code_parser.CodeParser","title":"<code>CodeParser</code>","text":"Source code in <code>src/preprocessing/code_parser.py</code> <pre><code>class CodeParser:\n    def __init__(self, repo_path: Path) -&gt; None:\n        \"\"\"\n        Initialize the CodeParser object with the path to the repository.\n        Sets the ignored patterns to gitignore patterns and default ignored extensions.\n\n        Args:\n            repo_path (Path): Path to the repository.\n        \"\"\"\n        self._repo_path = repo_path\n        self.ignored_patterns = (\n            self._load_gitignore_patterns() | DEFAULT_IGNORED_EXTENSIONS\n        )\n\n    def _load_gitignore_patterns(self) -&gt; set:\n        \"\"\"\n        Load the patterns from the .gitignore file in the repository.\n\n        Args:\n            repo_path (Path): Path to the repository.\n        Returns:\n            set: Set of patterns to be ignored if present, else empty set.\n        \"\"\"\n        gitignore_path = self._repo_path / \".gitignore\"\n        if not gitignore_path.exists():\n            return set()\n\n        with open(gitignore_path, \"r\") as f:\n            lines = f.readlines()\n\n        patterns = set()\n        for line in lines:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n\n            if line.startswith(\"!\"):\n                # Negation\n                line = line[1:]\n                patterns.discard(line)\n            else:\n                patterns.add(line)\n\n        return patterns\n\n    def _is_file_relevant(self, file_path: Path) -&gt; bool:\n        \"\"\"\n        Check if the file is relevant for the analysis.\n\n        Args:\n            file (Path): Path to the file.\n        Returns:\n            bool: True if the file is relevant, else False.\n        \"\"\"\n        if not file_path.is_file():\n            return False\n        if any(file_path.match(pattern) for pattern in self.ignored_patterns):\n            return False\n\n        try:\n            with file_path.open(\"rb\") as f:\n                chunk = f.read(1024)\n                if b\"\\0\" in chunk:\n                    return False\n            with file_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                return bool(f.readline())\n        except Exception:\n            return False\n\n    def get_relevant_files(self) -&gt; list[Path]:\n        \"\"\"\n        Get the code files in the repository.\n\n        Returns:\n            list[Path]: List of paths to the code files.\n        \"\"\"\n        return [\n            file for file in self._repo_path.rglob(\"*\") if self._is_file_relevant(file)\n        ]\n\n    def _classify_file(self, file_path: Path) -&gt; str:\n        \"\"\"\n        Classify the file as code, config or documentation.\n\n        Args:\n            file_path (Path): Path to the file.\n\n        Returns:\n            str: Type of the file.(code, config, documentation)\n        \"\"\"\n        ext = file_path.suffix\n        if ext in CONFIG_EXTENSIONS:\n            return \"config\"\n        if ext in DOC_EXTENSIONS:\n            return \"documentation\"\n        if ext in CODE_EXTENSIONS:\n            return \"code\"\n        return \"other\"\n\n    def _detect_language(self, file_path: Path) -&gt; str:\n        \"\"\"\n        Detect the language of the code file.\n\n        Args:\n            file_path (Path): Path to the code file.\n\n        Returns:\n            str: Language of the code file.\n        \"\"\"\n        return {\n            \".py\": \"Python\",\n            \".js\": \"JavaScript\",\n            \".ts\": \"TypeScript\",\n            \".vue\": \"Vue\",\n            \".cpp\": \"C++\",\n            \".java\": \"Java\",\n            \".html\": \"HTML\",\n            \".css\": \"CSS\",\n            \".sh\": \"Shell\",\n            \".sql\": \"SQL\",\n            \".r\": \"R\",\n            \".rb\": \"Ruby\",\n            \".php\": \"PHP\",\n            \".cs\": \"C#\",\n            \".go\": \"Go\",\n            \".md\": \"Markdown\",\n            \".yaml\": \"YAML\",\n            \".json\": \"JSON\",\n            \".xml\": \"XML\",\n        }.get(file_path.suffix, \"Unknown\")\n\n    def split_file(\n        self, file_path: Path, max_chunk_size: int, file_language: str\n    ) -&gt; list[tuple[str, int, int]]:\n        \"\"\"\n        Splits file into chunks.\n\n        Args:\n            file_path (Path): Path to the code file.\n            max_chunk_size (int): Maksimum chunk size.\n            file_language (str): language of file.\n\n        Returns:\n            list[tuple[str, int, int]]: List of Tuples in each tuple code lines, start line and end line\n        \"\"\"\n        block_pattern = LANGUAGE_PATTERNS.get(file_language)\n\n        with file_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            lines = f.readlines()\n\n        chunks = []\n        current_chunk = []\n        chunk_start_line = 0\n\n        for i, line in enumerate(lines):\n            current_chunk.append(line)\n\n            if block_pattern and re.match(block_pattern, line):\n                if len(current_chunk) &gt;= max_chunk_size:\n                    chunks.append(\n                        (\n                            \"\".join(current_chunk),\n                            chunk_start_line,\n                            i,\n                        )\n                    )\n                    current_chunk = []\n                    chunk_start_line = i\n\n        if current_chunk:\n            chunks.append(\n                (\n                    \"\".join(current_chunk),\n                    chunk_start_line,\n                    i,\n                )\n            )\n\n        return chunks\n\n    def parse(self, max_chunk_size: int) -&gt; list[dict]:\n        \"\"\"\n        Parse the code files in the repository.\n\n        Args:\n            max_chunk_size (int): Maximum size of the code chunk.\n        Returns:\n            list: List of code chunks.\n        \"\"\"\n        all_chunks: list[dict] = []\n        file_paths = self.get_relevant_files()\n        i = 0\n        for file in file_paths:\n            file_type = self._classify_file(file)\n            file_language = self._detect_language(file)\n            chunks = self.split_file(file, max_chunk_size, file_language)\n\n            for chunk in chunks:\n                all_chunks.append(\n                    {\n                        \"path\": str(file),\n                        \"relative_path\": str(file.relative_to(self._repo_path)),\n                        \"file_type\": file_type,\n                        \"language\": file_language,\n                        \"chunk_id\": i,\n                        \"code\": chunk[0],\n                        \"start_line\": chunk[1],\n                        \"end_line\": chunk[2],\n                    }\n                )\n                i += 1\n        return all_chunks\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.code_parser.CodeParser.__init__","title":"<code>__init__(repo_path)</code>","text":"<p>Initialize the CodeParser object with the path to the repository. Sets the ignored patterns to gitignore patterns and default ignored extensions.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to the repository.</p> required Source code in <code>src/preprocessing/code_parser.py</code> <pre><code>def __init__(self, repo_path: Path) -&gt; None:\n    \"\"\"\n    Initialize the CodeParser object with the path to the repository.\n    Sets the ignored patterns to gitignore patterns and default ignored extensions.\n\n    Args:\n        repo_path (Path): Path to the repository.\n    \"\"\"\n    self._repo_path = repo_path\n    self.ignored_patterns = (\n        self._load_gitignore_patterns() | DEFAULT_IGNORED_EXTENSIONS\n    )\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.code_parser.CodeParser.get_relevant_files","title":"<code>get_relevant_files()</code>","text":"<p>Get the code files in the repository.</p> <p>Returns:</p> Type Description <code>list[Path]</code> <p>list[Path]: List of paths to the code files.</p> Source code in <code>src/preprocessing/code_parser.py</code> <pre><code>def get_relevant_files(self) -&gt; list[Path]:\n    \"\"\"\n    Get the code files in the repository.\n\n    Returns:\n        list[Path]: List of paths to the code files.\n    \"\"\"\n    return [\n        file for file in self._repo_path.rglob(\"*\") if self._is_file_relevant(file)\n    ]\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.code_parser.CodeParser.parse","title":"<code>parse(max_chunk_size)</code>","text":"<p>Parse the code files in the repository.</p> <p>Parameters:</p> Name Type Description Default <code>max_chunk_size</code> <code>int</code> <p>Maximum size of the code chunk.</p> required <p>Returns:     list: List of code chunks.</p> Source code in <code>src/preprocessing/code_parser.py</code> <pre><code>def parse(self, max_chunk_size: int) -&gt; list[dict]:\n    \"\"\"\n    Parse the code files in the repository.\n\n    Args:\n        max_chunk_size (int): Maximum size of the code chunk.\n    Returns:\n        list: List of code chunks.\n    \"\"\"\n    all_chunks: list[dict] = []\n    file_paths = self.get_relevant_files()\n    i = 0\n    for file in file_paths:\n        file_type = self._classify_file(file)\n        file_language = self._detect_language(file)\n        chunks = self.split_file(file, max_chunk_size, file_language)\n\n        for chunk in chunks:\n            all_chunks.append(\n                {\n                    \"path\": str(file),\n                    \"relative_path\": str(file.relative_to(self._repo_path)),\n                    \"file_type\": file_type,\n                    \"language\": file_language,\n                    \"chunk_id\": i,\n                    \"code\": chunk[0],\n                    \"start_line\": chunk[1],\n                    \"end_line\": chunk[2],\n                }\n            )\n            i += 1\n    return all_chunks\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.code_parser.CodeParser.split_file","title":"<code>split_file(file_path, max_chunk_size, file_language)</code>","text":"<p>Splits file into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the code file.</p> required <code>max_chunk_size</code> <code>int</code> <p>Maksimum chunk size.</p> required <code>file_language</code> <code>str</code> <p>language of file.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, int, int]]</code> <p>list[tuple[str, int, int]]: List of Tuples in each tuple code lines, start line and end line</p> Source code in <code>src/preprocessing/code_parser.py</code> <pre><code>def split_file(\n    self, file_path: Path, max_chunk_size: int, file_language: str\n) -&gt; list[tuple[str, int, int]]:\n    \"\"\"\n    Splits file into chunks.\n\n    Args:\n        file_path (Path): Path to the code file.\n        max_chunk_size (int): Maksimum chunk size.\n        file_language (str): language of file.\n\n    Returns:\n        list[tuple[str, int, int]]: List of Tuples in each tuple code lines, start line and end line\n    \"\"\"\n    block_pattern = LANGUAGE_PATTERNS.get(file_language)\n\n    with file_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        lines = f.readlines()\n\n    chunks = []\n    current_chunk = []\n    chunk_start_line = 0\n\n    for i, line in enumerate(lines):\n        current_chunk.append(line)\n\n        if block_pattern and re.match(block_pattern, line):\n            if len(current_chunk) &gt;= max_chunk_size:\n                chunks.append(\n                    (\n                        \"\".join(current_chunk),\n                        chunk_start_line,\n                        i,\n                    )\n                )\n                current_chunk = []\n                chunk_start_line = i\n\n    if current_chunk:\n        chunks.append(\n            (\n                \"\".join(current_chunk),\n                chunk_start_line,\n                i,\n            )\n        )\n\n    return chunks\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.embedding_generator.EmbeddingGenerator","title":"<code>EmbeddingGenerator</code>","text":"<p>Creates embeddings for code chunks using SentenceTransformer</p> Source code in <code>src/preprocessing/embedding_generator.py</code> <pre><code>class EmbeddingGenerator:\n    \"\"\"Creates embeddings for code chunks using SentenceTransformer\"\"\"\n\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\") -&gt; None:\n        \"\"\"\n        Initializes the SentenceTransformer model.\n\n        Args:\n            model_name (str): Name of the SentenceTransformer model.\"\"\"\n        self.model = SentenceTransformer(model_name)\n\n    def create_embeddings(self, code_chunks: list[dict]) -&gt; list[dict]:\n        \"\"\"\n        Creates embeddings for code chunks\n\n        Args:\n            code_chunks (list): List of code chunks with metadata.\n\n        Returns:\n            list: List of code chunks with added embeddings.\n        \"\"\"\n        texts = [chunk[\"code\"].strip() for chunk in code_chunks]\n        embeddings = self.model.encode(texts, convert_to_numpy=True)\n\n        for i, chunk in enumerate(code_chunks):\n            chunk[\"embedding\"] = embeddings[i]\n\n        return code_chunks\n\n    def save_embeddings(self, code_chunks: list[dict], save_path: Path) -&gt; None:\n        \"\"\"\n        Saves embeddings and metadata to disk.\n        Embeddings are saved as a NumPy array, metadata is saved as JSON.\n\n        Args:\n            code_chunks (list): List of code chunks with metadata and embeddings.\n            save_path (Path): Path to save the embeddings and metadata.\n        \"\"\"\n\n        save_path.mkdir(parents=True, exist_ok=True)\n\n        metadata_path = (save_path / \"metadata\").with_suffix(\".json\")\n        embeddings_path = (save_path / \"embeddings\").with_suffix(\".npy\")\n        np.save(embeddings_path, np.array([c[\"embedding\"] for c in code_chunks]))\n\n        metadata = [\n            {k: v for k, v in c.items() if k != \"embedding\"} for c in code_chunks\n        ]\n        with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(metadata, f, indent=2, ensure_ascii=False)\n\n    def load_embeddings(self, save_path: Path) -&gt; list[dict]:\n        \"\"\"\n        Loads embeddings and metadata from disk.\n\n        Args:\n            save_path (Path): Path to the embeddings and metadata.\n\n        Returns:\n            list: List of code chunks with metadata and embeddings.\n        \"\"\"\n        metadata_path = (save_path / \"metadata\").with_suffix(\".json\")\n        embeddings_path = (save_path / \"embeddings\").with_suffix(\".npy\")\n\n        with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n\n        embeddings = np.load(embeddings_path)\n\n        for i, chunk in enumerate(metadata):\n            chunk[\"embedding\"] = embeddings[i]\n\n        return metadata\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.embedding_generator.EmbeddingGenerator.__init__","title":"<code>__init__(model_name='all-MiniLM-L6-v2')</code>","text":"<p>Initializes the SentenceTransformer model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the SentenceTransformer model.</p> <code>'all-MiniLM-L6-v2'</code> Source code in <code>src/preprocessing/embedding_generator.py</code> <pre><code>def __init__(self, model_name: str = \"all-MiniLM-L6-v2\") -&gt; None:\n    \"\"\"\n    Initializes the SentenceTransformer model.\n\n    Args:\n        model_name (str): Name of the SentenceTransformer model.\"\"\"\n    self.model = SentenceTransformer(model_name)\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.embedding_generator.EmbeddingGenerator.create_embeddings","title":"<code>create_embeddings(code_chunks)</code>","text":"<p>Creates embeddings for code chunks</p> <p>Parameters:</p> Name Type Description Default <code>code_chunks</code> <code>list</code> <p>List of code chunks with metadata.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[dict]</code> <p>List of code chunks with added embeddings.</p> Source code in <code>src/preprocessing/embedding_generator.py</code> <pre><code>def create_embeddings(self, code_chunks: list[dict]) -&gt; list[dict]:\n    \"\"\"\n    Creates embeddings for code chunks\n\n    Args:\n        code_chunks (list): List of code chunks with metadata.\n\n    Returns:\n        list: List of code chunks with added embeddings.\n    \"\"\"\n    texts = [chunk[\"code\"].strip() for chunk in code_chunks]\n    embeddings = self.model.encode(texts, convert_to_numpy=True)\n\n    for i, chunk in enumerate(code_chunks):\n        chunk[\"embedding\"] = embeddings[i]\n\n    return code_chunks\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.embedding_generator.EmbeddingGenerator.load_embeddings","title":"<code>load_embeddings(save_path)</code>","text":"<p>Loads embeddings and metadata from disk.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>Path</code> <p>Path to the embeddings and metadata.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[dict]</code> <p>List of code chunks with metadata and embeddings.</p> Source code in <code>src/preprocessing/embedding_generator.py</code> <pre><code>def load_embeddings(self, save_path: Path) -&gt; list[dict]:\n    \"\"\"\n    Loads embeddings and metadata from disk.\n\n    Args:\n        save_path (Path): Path to the embeddings and metadata.\n\n    Returns:\n        list: List of code chunks with metadata and embeddings.\n    \"\"\"\n    metadata_path = (save_path / \"metadata\").with_suffix(\".json\")\n    embeddings_path = (save_path / \"embeddings\").with_suffix(\".npy\")\n\n    with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n        metadata = json.load(f)\n\n    embeddings = np.load(embeddings_path)\n\n    for i, chunk in enumerate(metadata):\n        chunk[\"embedding\"] = embeddings[i]\n\n    return metadata\n</code></pre>"},{"location":"api/preprocessing/#src.preprocessing.embedding_generator.EmbeddingGenerator.save_embeddings","title":"<code>save_embeddings(code_chunks, save_path)</code>","text":"<p>Saves embeddings and metadata to disk. Embeddings are saved as a NumPy array, metadata is saved as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>code_chunks</code> <code>list</code> <p>List of code chunks with metadata and embeddings.</p> required <code>save_path</code> <code>Path</code> <p>Path to save the embeddings and metadata.</p> required Source code in <code>src/preprocessing/embedding_generator.py</code> <pre><code>def save_embeddings(self, code_chunks: list[dict], save_path: Path) -&gt; None:\n    \"\"\"\n    Saves embeddings and metadata to disk.\n    Embeddings are saved as a NumPy array, metadata is saved as JSON.\n\n    Args:\n        code_chunks (list): List of code chunks with metadata and embeddings.\n        save_path (Path): Path to save the embeddings and metadata.\n    \"\"\"\n\n    save_path.mkdir(parents=True, exist_ok=True)\n\n    metadata_path = (save_path / \"metadata\").with_suffix(\".json\")\n    embeddings_path = (save_path / \"embeddings\").with_suffix(\".npy\")\n    np.save(embeddings_path, np.array([c[\"embedding\"] for c in code_chunks]))\n\n    metadata = [\n        {k: v for k, v in c.items() if k != \"embedding\"} for c in code_chunks\n    ]\n    with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, indent=2, ensure_ascii=False)\n</code></pre>"},{"location":"api/retriver/","title":"Retriver","text":""},{"location":"api/retriver/#src.retriever.faiss_search.FAISSRetriever","title":"<code>FAISSRetriever</code>","text":"Source code in <code>src/retriever/faiss_search.py</code> <pre><code>class FAISSRetriever:\n    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\") -&gt; None:\n        \"\"\"\n        Initializes the SentenceTransformer model and the FAISS index.\n\n        Args:\n            embedding_model (str): Name of the SentenceTransformer model.\n        \"\"\"\n        self.model = SentenceTransformer(embedding_model)\n        self.index: faiss = None\n        self.metadata: list[dict] = []\n\n    def load_index(self, project_name: str) -&gt; None:\n        \"\"\"\n        Loads the FAISS index and metadata from disk.\n\n        Args:\n            project_name (str): Name of the project.\n        \"\"\"\n        index_path = (Path(FAISS_INDEX_DIR) / project_name).with_suffix(\".faiss\")\n        metadata_path = (Path(EMBEDDINGS_DIR) / project_name / \"metadata\").with_suffix(\n            \".json\"\n        )\n\n        self.index = faiss.read_index(str(index_path))\n\n        with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n            self.metadata = json.load(f)\n\n    def search(\n        self,\n        query: str,\n        radius: float = None,\n        top_k: int = None,\n        expand_query_type: str = None,\n        rerank: bool = False,\n        similarity_threshold: float = 0.5,\n        query_top_k: int = 7,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Searches the FAISS index for code chunks based on either a similarity radius or top_k results.\n\n        Args:\n            query (str): Query string.\n            radius (float, optional): Radius for similarity search .\n            top_k (int, optional): Number of top results to retrieve.\n            expand_query_type (str, optional): Type of expand query technique(None, wordnet, candidate_terms)\n            rerank (bool): Use reranker or not\n            similarity_threshold (float): similarity threshold for reranker\n            query_top_k (int): top k for query expansion\n\n        Returns:\n            list: List of similar code chunks with metadata.\n        \"\"\"\n        if radius is None and top_k is None:\n            raise ValueError(\"Either 'radius' or 'top_k' must be specified.\")\n\n        if expand_query_type == \"wordnet\":\n            query = QueryExpander.expand_query_with_wordnet(query=query)\n        elif expand_query_type == \"candidate_terms\":\n            query = QueryExpander.expand_query_with_embeddings(\n                query=query, model=self.model, top_k=query_top_k\n            )\n\n        query_embedding = self.model.encode([query], convert_to_numpy=True)\n\n        results = []\n        files = set()\n\n        if radius is not None:\n            lims, distances, indices = self.index.range_search(query_embedding, radius)\n            for i in range(len(lims) - 1):\n                for idx, dist in zip(\n                    indices[lims[i] : lims[i + 1]], distances[lims[i] : lims[i + 1]]\n                ):\n                    if (\n                        self.metadata[idx][\"file_type\"] != \"other\"\n                        and self.metadata[idx][\"relative_path\"] not in files\n                    ):\n                        results.append((self.metadata[idx], dist))\n                        files.add(self.metadata[idx][\"relative_path\"])\n        elif top_k is not None:\n            distances, indices = self.index.search(query_embedding, top_k)\n            for idx, dist in zip(indices[0], distances[0]):\n                if (\n                    self.metadata[idx][\"file_type\"] != \"other\"\n                    and self.metadata[idx][\"relative_path\"] not in files\n                ):\n                    results.append((self.metadata[idx], dist))\n                    files.add(self.metadata[idx][\"relative_path\"])\n\n        if rerank:\n            self.rerank(query_embedding, results, similarity_threshold)\n        else:\n            results.sort(key=lambda x: x[1])\n\n        return [metadata for metadata, _ in results]\n\n    def rerank(\n        self,\n        query_embedding: np.ndarray,\n        results: list[dict],\n        similarity_threshold: float,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Reranks the search results based on the cosine similarity between the query embedding and the code chunks.\n\n        Args:\n            query_embedding (np.ndarray): Precomputed embedding of the query.\n            results (list): List of search results with metadata.\n            similarity_threshold (float): Minimum cosine similarity score to include a result.\n\n        Returns:\n            list: List of reranked search results with metadata.\n        \"\"\"\n        code_chunks = [res[0][\"code\"] for res in results]\n        code_embeddings = self.model.encode(code_chunks, convert_to_numpy=True)\n\n        scores = cosine_similarity(query_embedding, code_embeddings)[0]\n\n        filtered_results = [\n            (res, score)\n            for res, score in zip(results, scores)\n            if score &gt;= similarity_threshold\n        ]\n        filtered_results = sorted(filtered_results, key=lambda x: x[1], reverse=True)\n\n        return filtered_results\n\n    def build_index(self, project_name: str) -&gt; None:\n        \"\"\"\n        Builds the FAISS index for the project.\n\n        Args:\n            project_name (str): Name of the project.\n        \"\"\"\n        embeddings_path = (\n            Path(EMBEDDINGS_DIR) / project_name / \"embeddings\"\n        ).with_suffix(\".npy\")\n        metadata_path = (Path(EMBEDDINGS_DIR) / project_name / \"metadata\").with_suffix(\n            \".json\"\n        )\n\n        embeddings = np.load(embeddings_path)\n        self.metadata = json.loads(metadata_path.read_text())\n\n        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n        self.index.add(embeddings)\n\n        index_path = (Path(FAISS_INDEX_DIR) / project_name).with_suffix(\".faiss\")\n        index_path.parent.mkdir(parents=True, exist_ok=True)\n        faiss.write_index(self.index, str(index_path))\n</code></pre>"},{"location":"api/retriver/#src.retriever.faiss_search.FAISSRetriever.__init__","title":"<code>__init__(embedding_model='all-MiniLM-L6-v2')</code>","text":"<p>Initializes the SentenceTransformer model and the FAISS index.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>str</code> <p>Name of the SentenceTransformer model.</p> <code>'all-MiniLM-L6-v2'</code> Source code in <code>src/retriever/faiss_search.py</code> <pre><code>def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\") -&gt; None:\n    \"\"\"\n    Initializes the SentenceTransformer model and the FAISS index.\n\n    Args:\n        embedding_model (str): Name of the SentenceTransformer model.\n    \"\"\"\n    self.model = SentenceTransformer(embedding_model)\n    self.index: faiss = None\n    self.metadata: list[dict] = []\n</code></pre>"},{"location":"api/retriver/#src.retriever.faiss_search.FAISSRetriever.build_index","title":"<code>build_index(project_name)</code>","text":"<p>Builds the FAISS index for the project.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the project.</p> required Source code in <code>src/retriever/faiss_search.py</code> <pre><code>def build_index(self, project_name: str) -&gt; None:\n    \"\"\"\n    Builds the FAISS index for the project.\n\n    Args:\n        project_name (str): Name of the project.\n    \"\"\"\n    embeddings_path = (\n        Path(EMBEDDINGS_DIR) / project_name / \"embeddings\"\n    ).with_suffix(\".npy\")\n    metadata_path = (Path(EMBEDDINGS_DIR) / project_name / \"metadata\").with_suffix(\n        \".json\"\n    )\n\n    embeddings = np.load(embeddings_path)\n    self.metadata = json.loads(metadata_path.read_text())\n\n    self.index = faiss.IndexFlatIP(embeddings.shape[1])\n    self.index.add(embeddings)\n\n    index_path = (Path(FAISS_INDEX_DIR) / project_name).with_suffix(\".faiss\")\n    index_path.parent.mkdir(parents=True, exist_ok=True)\n    faiss.write_index(self.index, str(index_path))\n</code></pre>"},{"location":"api/retriver/#src.retriever.faiss_search.FAISSRetriever.load_index","title":"<code>load_index(project_name)</code>","text":"<p>Loads the FAISS index and metadata from disk.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the project.</p> required Source code in <code>src/retriever/faiss_search.py</code> <pre><code>def load_index(self, project_name: str) -&gt; None:\n    \"\"\"\n    Loads the FAISS index and metadata from disk.\n\n    Args:\n        project_name (str): Name of the project.\n    \"\"\"\n    index_path = (Path(FAISS_INDEX_DIR) / project_name).with_suffix(\".faiss\")\n    metadata_path = (Path(EMBEDDINGS_DIR) / project_name / \"metadata\").with_suffix(\n        \".json\"\n    )\n\n    self.index = faiss.read_index(str(index_path))\n\n    with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n        self.metadata = json.load(f)\n</code></pre>"},{"location":"api/retriver/#src.retriever.faiss_search.FAISSRetriever.rerank","title":"<code>rerank(query_embedding, results, similarity_threshold)</code>","text":"<p>Reranks the search results based on the cosine similarity between the query embedding and the code chunks.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>ndarray</code> <p>Precomputed embedding of the query.</p> required <code>results</code> <code>list</code> <p>List of search results with metadata.</p> required <code>similarity_threshold</code> <code>float</code> <p>Minimum cosine similarity score to include a result.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[dict]</code> <p>List of reranked search results with metadata.</p> Source code in <code>src/retriever/faiss_search.py</code> <pre><code>def rerank(\n    self,\n    query_embedding: np.ndarray,\n    results: list[dict],\n    similarity_threshold: float,\n) -&gt; list[dict]:\n    \"\"\"\n    Reranks the search results based on the cosine similarity between the query embedding and the code chunks.\n\n    Args:\n        query_embedding (np.ndarray): Precomputed embedding of the query.\n        results (list): List of search results with metadata.\n        similarity_threshold (float): Minimum cosine similarity score to include a result.\n\n    Returns:\n        list: List of reranked search results with metadata.\n    \"\"\"\n    code_chunks = [res[0][\"code\"] for res in results]\n    code_embeddings = self.model.encode(code_chunks, convert_to_numpy=True)\n\n    scores = cosine_similarity(query_embedding, code_embeddings)[0]\n\n    filtered_results = [\n        (res, score)\n        for res, score in zip(results, scores)\n        if score &gt;= similarity_threshold\n    ]\n    filtered_results = sorted(filtered_results, key=lambda x: x[1], reverse=True)\n\n    return filtered_results\n</code></pre>"},{"location":"api/retriver/#src.retriever.faiss_search.FAISSRetriever.search","title":"<code>search(query, radius=None, top_k=None, expand_query_type=None, rerank=False, similarity_threshold=0.5, query_top_k=7)</code>","text":"<p>Searches the FAISS index for code chunks based on either a similarity radius or top_k results.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query string.</p> required <code>radius</code> <code>float</code> <p>Radius for similarity search .</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Number of top results to retrieve.</p> <code>None</code> <code>expand_query_type</code> <code>str</code> <p>Type of expand query technique(None, wordnet, candidate_terms)</p> <code>None</code> <code>rerank</code> <code>bool</code> <p>Use reranker or not</p> <code>False</code> <code>similarity_threshold</code> <code>float</code> <p>similarity threshold for reranker</p> <code>0.5</code> <code>query_top_k</code> <code>int</code> <p>top k for query expansion</p> <code>7</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[dict]</code> <p>List of similar code chunks with metadata.</p> Source code in <code>src/retriever/faiss_search.py</code> <pre><code>def search(\n    self,\n    query: str,\n    radius: float = None,\n    top_k: int = None,\n    expand_query_type: str = None,\n    rerank: bool = False,\n    similarity_threshold: float = 0.5,\n    query_top_k: int = 7,\n) -&gt; list[dict]:\n    \"\"\"\n    Searches the FAISS index for code chunks based on either a similarity radius or top_k results.\n\n    Args:\n        query (str): Query string.\n        radius (float, optional): Radius for similarity search .\n        top_k (int, optional): Number of top results to retrieve.\n        expand_query_type (str, optional): Type of expand query technique(None, wordnet, candidate_terms)\n        rerank (bool): Use reranker or not\n        similarity_threshold (float): similarity threshold for reranker\n        query_top_k (int): top k for query expansion\n\n    Returns:\n        list: List of similar code chunks with metadata.\n    \"\"\"\n    if radius is None and top_k is None:\n        raise ValueError(\"Either 'radius' or 'top_k' must be specified.\")\n\n    if expand_query_type == \"wordnet\":\n        query = QueryExpander.expand_query_with_wordnet(query=query)\n    elif expand_query_type == \"candidate_terms\":\n        query = QueryExpander.expand_query_with_embeddings(\n            query=query, model=self.model, top_k=query_top_k\n        )\n\n    query_embedding = self.model.encode([query], convert_to_numpy=True)\n\n    results = []\n    files = set()\n\n    if radius is not None:\n        lims, distances, indices = self.index.range_search(query_embedding, radius)\n        for i in range(len(lims) - 1):\n            for idx, dist in zip(\n                indices[lims[i] : lims[i + 1]], distances[lims[i] : lims[i + 1]]\n            ):\n                if (\n                    self.metadata[idx][\"file_type\"] != \"other\"\n                    and self.metadata[idx][\"relative_path\"] not in files\n                ):\n                    results.append((self.metadata[idx], dist))\n                    files.add(self.metadata[idx][\"relative_path\"])\n    elif top_k is not None:\n        distances, indices = self.index.search(query_embedding, top_k)\n        for idx, dist in zip(indices[0], distances[0]):\n            if (\n                self.metadata[idx][\"file_type\"] != \"other\"\n                and self.metadata[idx][\"relative_path\"] not in files\n            ):\n                results.append((self.metadata[idx], dist))\n                files.add(self.metadata[idx][\"relative_path\"])\n\n    if rerank:\n        self.rerank(query_embedding, results, similarity_threshold)\n    else:\n        results.sort(key=lambda x: x[1])\n\n    return [metadata for metadata, _ in results]\n</code></pre>"},{"location":"api/retriver/#src.retriever.query_expander.QueryExpander","title":"<code>QueryExpander</code>","text":"Source code in <code>src/retriever/query_expander.py</code> <pre><code>class QueryExpander:\n    @staticmethod\n    def expand_query_with_embeddings(\n        query: str, model: SentenceTransformer, top_k: int = 5\n    ) -&gt; str:\n        \"\"\"\n        Expands the query by finding semantically similar terms using embeddings.\n\n        Args:\n            query (str): Original query.\n            model (SentenceTransformer): Pretrained SentenceTransformer model.\n            top_k (int): Number of similar terms to add.\n\n        Returns:\n            str: Expanded query.\n        \"\"\"\n\n        query_embedding = model.encode([query], convert_to_numpy=True)\n        candidate_embeddings = model.encode(candidate_terms, convert_to_numpy=True)\n        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n\n        top_indices = np.argsort(similarities)[-top_k:]\n        expanded_terms = [candidate_terms[i] for i in top_indices]\n\n        return query + \" \" + \" \".join(expanded_terms)\n\n    @staticmethod\n    def expand_query_with_wordnet(query: str) -&gt; str:\n        \"\"\"\n        Expands the query using WordNet to find synonyms.\n\n        Args:\n            query (str): Original query.\n\n        Returns:\n            str: Expanded query.\n        \"\"\"\n        expanded_terms = []\n        for word in query.split():\n            synonyms = wordnet.synsets(word)\n            for syn in synonyms:\n                for lemma in syn.lemmas():\n                    expanded_terms.append(lemma.name())\n\n        return query + \" \" + \" \".join(set(expanded_terms))\n</code></pre>"},{"location":"api/retriver/#src.retriever.query_expander.QueryExpander.expand_query_with_embeddings","title":"<code>expand_query_with_embeddings(query, model, top_k=5)</code>  <code>staticmethod</code>","text":"<p>Expands the query by finding semantically similar terms using embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Original query.</p> required <code>model</code> <code>SentenceTransformer</code> <p>Pretrained SentenceTransformer model.</p> required <code>top_k</code> <code>int</code> <p>Number of similar terms to add.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Expanded query.</p> Source code in <code>src/retriever/query_expander.py</code> <pre><code>@staticmethod\ndef expand_query_with_embeddings(\n    query: str, model: SentenceTransformer, top_k: int = 5\n) -&gt; str:\n    \"\"\"\n    Expands the query by finding semantically similar terms using embeddings.\n\n    Args:\n        query (str): Original query.\n        model (SentenceTransformer): Pretrained SentenceTransformer model.\n        top_k (int): Number of similar terms to add.\n\n    Returns:\n        str: Expanded query.\n    \"\"\"\n\n    query_embedding = model.encode([query], convert_to_numpy=True)\n    candidate_embeddings = model.encode(candidate_terms, convert_to_numpy=True)\n    similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n\n    top_indices = np.argsort(similarities)[-top_k:]\n    expanded_terms = [candidate_terms[i] for i in top_indices]\n\n    return query + \" \" + \" \".join(expanded_terms)\n</code></pre>"},{"location":"api/retriver/#src.retriever.query_expander.QueryExpander.expand_query_with_wordnet","title":"<code>expand_query_with_wordnet(query)</code>  <code>staticmethod</code>","text":"<p>Expands the query using WordNet to find synonyms.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Original query.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Expanded query.</p> Source code in <code>src/retriever/query_expander.py</code> <pre><code>@staticmethod\ndef expand_query_with_wordnet(query: str) -&gt; str:\n    \"\"\"\n    Expands the query using WordNet to find synonyms.\n\n    Args:\n        query (str): Original query.\n\n    Returns:\n        str: Expanded query.\n    \"\"\"\n    expanded_terms = []\n    for word in query.split():\n        synonyms = wordnet.synsets(word)\n        for syn in synonyms:\n            for lemma in syn.lemmas():\n                expanded_terms.append(lemma.name())\n\n    return query + \" \" + \" \".join(set(expanded_terms))\n</code></pre>"},{"location":"api/scripts/","title":"Main Scripts","text":""},{"location":"api/scripts/#main-scripts","title":"Main Scripts","text":""},{"location":"api/scripts/#src.knowledge_base_preparation.download_repo","title":"<code>download_repo(repo_url)</code>","text":"<p>Downloads the repository from the given URL.</p> Source code in <code>src/knowledge_base_preparation.py</code> <pre><code>def download_repo(repo_url: str) -&gt; str:\n    \"\"\"\n    Downloads the repository from the given URL.\n    \"\"\"\n    repo_loader = RepoLoader(repo_url)\n    repo = repo_loader.clone()\n    return repo.working_dir\n</code></pre>"},{"location":"api/scripts/#src.knowledge_base_preparation.prepare_knowledge_base","title":"<code>prepare_knowledge_base(max_chunk_size, repo_path)</code>","text":"<p>Script for preparing the knowledge base. Includes:     - parsing the code     - generating embeddings     - building the FAISS index.</p> <p>Configurations can be found in src/config.py.</p> <p>Parameters:</p> Name Type Description Default <code>max_chunk_size</code> <code>int</code> <p>Maximum size of the code chunks</p> required <code>repo_path</code> <code>str</code> <p>Path to the repository</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/knowledge_base_preparation.py</code> <pre><code>def prepare_knowledge_base(max_chunk_size: int, repo_path: str) -&gt; None:\n    \"\"\"\n    Script for preparing the knowledge base.\n    Includes:\n        - parsing the code\n        - generating embeddings\n        - building the FAISS index.\n\n    Configurations can be found in src/config.py.\n\n    Args:\n        max_chunk_size (int): Maximum size of the code chunks\n        repo_path (str): Path to the repository\n\n    Returns:\n        None\n    \"\"\"\n\n    code_parser = CodeParser(Path(repo_path))\n    code_chunks = code_parser.parse(max_chunk_size)\n\n    project_name = repo_path.split(\"/\")[-1]\n\n    embedding_generator = EmbeddingGenerator()\n    code_chunks = embedding_generator.create_embeddings(code_chunks)\n    embedding_generator.save_embeddings(\n        code_chunks, save_path=Path(EMBEDDINGS_DIR) / project_name\n    )\n\n    retriever = FAISSRetriever()\n    retriever.build_index(project_name)\n</code></pre>"},{"location":"api/scripts/#src.main.main","title":"<code>main()</code>","text":"<p>Streamlit application for code search with optional LLM summarization. To run this script 'streamlit run src/main.py --server.fileWatcherType=none'</p> Source code in <code>src/main.py</code> <pre><code>def main():\n    \"\"\"\n    Streamlit application for code search with optional LLM summarization.\n    To run this script 'streamlit run src/main.py --server.fileWatcherType=none'\n    \"\"\"\n    retriever = FAISSRetriever(embedding_model=EMBEDDING_MODEL)\n    retriever.load_index(\"escrcpy\")\n\n    st.title(\"Code Search Application\")\n\n    search_type = st.radio(\n        \"Select the type of search you want to perform:\",\n        (\"Radius search\", \"Top K search\"),\n    )\n\n    if search_type == \"Radius search\":\n        radius = st.slider(\n            \"Select radius for search:\", min_value=0.1, max_value=1.0, step=0.05, value=0.26\n        )\n        expand_query_type = st.selectbox(\n            \"Select query expansion type:\", (\"None\", \"WordNet\", \"Candidate Terms\"), index=2\n        )\n    elif search_type == \"Top K search\":\n        top_k = st.slider(\n            \"Select the number of top results:\", min_value=1, max_value=20, step=1, value=11\n        )\n        expand_query_type = st.selectbox(\n            \"Select query expansion type:\", (\"None\", \"WordNet\", \"Candidate Terms\"), index=2\n        )\n\n    query = st.text_input(\"Enter your query:\")\n    use_llm = st.checkbox(\"\ud83e\udd16 Use LLM explanation\", value=False, help=\"Generates a natural language summary\")\n\n    if st.button(\"Search\"):\n        if query.strip() == \"\":\n            st.warning(\"Please enter a query.\")\n        else:\n            if search_type == \"Radius search\":\n                expand_query = None\n                if expand_query_type == \"WordNet\":\n                    expand_query = \"wordnet\"\n                elif expand_query_type == \"Candidate Terms\":\n                    expand_query = \"candidate_terms\"\n                results = retriever.search(\n                    query, radius=radius, expand_query_type=expand_query\n                )\n            elif search_type == \"Top K search\":\n                expand_query = None\n                if expand_query_type == \"WordNet\":\n                    expand_query = \"wordnet\"\n                elif expand_query_type == \"Candidate Terms\":\n                    expand_query = \"candidate_terms\"\n\n                results = retriever.search(\n                    query, top_k=top_k, expand_query_type=expand_query\n                )\n\n            if results:\n                st.success(f\"Found {len(results)} results:\")\n                for result in results:\n                    st.markdown(f\"- **{result['relative_path']}**\")\n\n                if use_llm:\n                    if not LLM_AVAILABLE:\n                        st.error(\"LLM dependencies are not installed.\\nRun `make requirements-llm` to enable this feature.\")\n                    else:\n                        with st.spinner(\"Generating explanation with LLM...\"):\n                            explanation = generate_summary(query, results)\n                            st.markdown(\"### \ud83e\udd16 LLM Explanation\")\n                            st.info(explanation)\n            else:\n                st.warning(\"No results found.\")\n</code></pre>"}]}